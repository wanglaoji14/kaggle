{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbc0a79f-56af-4189-a87c-dd9febc98b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/myproject/lib/python3.11/site-packages (1.7.1)\n",
      "Collecting xgboost==3.0.3\n",
      "  Using cached xgboost-3.0.3-py3-none-macosx_12_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: lightgbm==4.6.0 in /opt/anaconda3/envs/myproject/lib/python3.11/site-packages (4.6.0)\n",
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Collecting scipy==1.14.1\n",
      "  Downloading scipy-1.14.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/myproject/lib/python3.11/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/myproject/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Using cached xgboost-3.0.3-py3-none-macosx_12_0_arm64.whl (2.0 MB)\n",
      "Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp311-cp311-macosx_14_0_arm64.whl (23.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, scipy, xgboost\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 2.3.2\n",
      "\u001b[2K    Uninstalling numpy-2.3.2:\n",
      "\u001b[2K      Successfully uninstalled numpy-2.3.2\n",
      "\u001b[2K  Attempting uninstall: scipy━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/3\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: scipy 1.16.1[0m \u001b[32m0/3\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling scipy-1.16.1:━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/3\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled scipy-1.16.1━\u001b[0m \u001b[32m0/3\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: xgboost\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [scipy]\n",
      "\u001b[2K    Found existing installation: xgboost 2.1.1━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling xgboost-2.1.1:\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled xgboost-2.1.1━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [xgboost]m2/3\u001b[0m [xgboost]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas-profiling 3.2.0 requires joblib~=1.1.0, but you have joblib 1.5.1 which is incompatible.\n",
      "pandas-profiling 3.2.0 requires visions[type_image_path]==0.7.4, but you have visions 0.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4 scipy-1.14.1 xgboost-3.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade scikit-learn scikit-learn==1.7.1 xgboost==3.0.3 lightgbm==4.6.0 numpy==1.26.4 scipy==1.14.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2eb30fb-a95f-4183-bbdf-72677a8366f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder, label_binarize, OrdinalEncoder, QuantileTransformer, TargetEncoder\n",
    "from category_encoders import CatBoostEncoder, MEstimateEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression, LinearRegression, BayesianRidge, Ridge\n",
    "\n",
    "from sklearn import set_config\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, root_mean_squared_error, mean_squared_error, precision_recall_curve, make_scorer, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, matthews_corrcoef\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "from colorama import Fore, Style, init\n",
    "from copy import deepcopy\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, StratifiedKFold, KFold, RepeatedKFold, cross_val_score, StratifiedGroupKFold\n",
    "import xgboost as xgb\n",
    "from xgboost import DMatrix, XGBClassifier, XGBRegressor\n",
    "\n",
    "from lightgbm import log_evaluation, early_stopping, LGBMClassifier, LGBMRegressor, Dataset\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
    "from tqdm.notebook import tqdm\n",
    "from optuna.samplers import TPESampler, CmaEsSampler\n",
    "from optuna.pruners import HyperbandPruner\n",
    "from functools import partial\n",
    "from IPython.display import display_html, clear_output\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import gc\n",
    "import re\n",
    "from typing import Literal, NamedTuple\n",
    "from itertools import combinations\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a843c72b-cd8b-4ae5-87ff-f26fb593e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#为预处理数据\n",
    "class Config:\n",
    "\n",
    "        state = 42\n",
    "        n_splits = 10\n",
    "        early_stop = 100\n",
    "        \n",
    "        target = 'y'\n",
    "        train = pd.read_csv(\"data/train.csv\")\n",
    "        test = pd.read_csv(\"data/test.csv\")\n",
    "        submission =pd.read_csv(\"data/sample_submission.csv\")\n",
    "    \n",
    "        original_data = False\n",
    "        outliers = False\n",
    "        log_trf = False\n",
    "        feature_eng = True\n",
    "        missing = False\n",
    "        labels = list(train[target].unique())\n",
    "        topk_interactions = 20\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3edfbdf-6da3-4802-bdd7-8dbc872dd831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class Transform(Config):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 调用父类 Config 的 __init__ 方法（如果存在）\n",
    "        super().__init__()\n",
    "        t0 = time.time()\n",
    "        # 如果启用 original_data，则将原始数据合并到训练集\n",
    "        if self.original_data:\n",
    "            start = time.time()\n",
    "            # 将目标列转换为 0/1（假设值为 \"yes\" 和 \"no\"）\n",
    "            self.train_org[self.target] = (self.train_org[self.target] == \"yes\").astype(int)\n",
    "            # 合并并去重\n",
    "            self.train = pd.concat([self.train, self.train_org], ignore_index=True).drop_duplicates()\n",
    "            self.train.reset_index(drop=True, inplace=True)\n",
    "            print(f\"[合并原始数据] {time.time()-start:.2f}s\")\n",
    "\n",
    "        \n",
    "        # 获取数值型特征列名（排除 object/bool/category/string）\n",
    "        self.num_features = self.train.drop(self.target, axis=1)\\\n",
    "            .select_dtypes(exclude=['object', 'bool', 'category', 'string']).columns.tolist()\n",
    "        \n",
    "        # 获取类别特征列名（只保留 object/bool/category/string）\n",
    "        self.cat_features = self.train.drop(self.target, axis=1)\\\n",
    "            .select_dtypes(include=['object', 'bool', 'category', 'string']).columns.tolist()\n",
    "\n",
    "        if self.missing:\n",
    "            self.missing_values()\n",
    "\n",
    "        if self.outliers:\n",
    "            self.remove_outliers()\n",
    "\n",
    "        if self.log_trf:\n",
    "            self.log_transformation()\n",
    "\n",
    "\n",
    "        start = time.time()\n",
    "        self.important_features = self.select_important_features(top_k=20)\n",
    "        print(f\"[特征重要度] {time.time()-start:.2f}s\")\n",
    "\n",
    "        if self.feature_eng and self.important_features:\n",
    "            self.train = self.new_features(self.train, self.important_features)\n",
    "            self.test  = self.new_features(self.test, self.important_features)\n",
    "            self.num_features = self.train.drop(self.target, axis=1)\\\n",
    "                .select_dtypes(exclude=['object', 'bool', 'string', 'category']).columns.tolist()\n",
    "            print(f\"[交互特征] {time.time()-start:.2f}s\")\n",
    "\n",
    "        start = time.time()\n",
    "        self.encode()\n",
    "\n",
    "        print(f\"[总耗时] {time.time()-t0:.2f}s\")\n",
    "        \n",
    "    def __call__(self):\n",
    "        # 保存目标列\n",
    "        self.y = self.train[self.target]\n",
    "        # 保存原始特征\n",
    "        self.X = self.train.drop(self.target, axis=1)\n",
    "        # 保存编码后的特征\n",
    "        self.X_enc = self.train_enc.drop(self.target, axis=1)\n",
    "        return self.X, self.X_enc, self.y, self.test, self.test_enc, self.cat_features, self.num_features\n",
    "    \n",
    "    def encode(self):\n",
    "        self.train_enc = self.train.copy()\n",
    "        self.test_enc = self.test.copy()\n",
    "        \n",
    "        self.cat_features_card = []\n",
    "        for f in self.cat_features:\n",
    "            self.cat_features_card.append(self.train[f].nunique())\n",
    "        \n",
    "        # 创建编码器并仅用训练集类别特征进行 fit\n",
    "        oe = OrdinalEncoder()\n",
    "        oe.fit(self.train_enc[self.cat_features])\n",
    "        \n",
    "        # 分别对训练集和测试集进行 transform\n",
    "        self.train_enc[self.cat_features] = oe.transform(self.train_enc[self.cat_features]).astype(int)\n",
    "        self.test_enc[self.cat_features] = oe.transform(self.test_enc[self.cat_features]).astype(int)\n",
    "        \n",
    "        # 创建标准化器并仅用训练集数值特征进行 fit\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(self.train_enc[self.num_features])\n",
    "\n",
    "        \n",
    "        # 分别对训练集和测试集进行 transform\n",
    "        self.train_enc[self.num_features] = scaler.transform(self.train_enc[self.num_features])\n",
    "        self.test_enc[self.num_features] = scaler.transform(self.test_enc[self.num_features])\n",
    "\n",
    "    def select_important_features(self, top_k=20, task='auto'):\n",
    "        \"\"\"\n",
    "        基于树模型的特征重要度筛选，并记录耗时。\n",
    "        功能：返回用于生成交互项的前 top_k 个重要数值特征。\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Step 1: 构造候选列\n",
    "        feat_cols = [c for c in self.num_features if c in self.train.columns and c != self.target]\n",
    "        if not feat_cols:\n",
    "            print(\"[select_important_features] 无可用数值特征，返回空。\")\n",
    "            return []\n",
    "\n",
    "\n",
    "        # Step 2: 准备训练数据\n",
    "        X_train_imp = self.train[feat_cols]\n",
    "        y_train_imp = self.train[self.target]\n",
    "        is_class = (y_train_imp.nunique() <= 10)\n",
    "\n",
    "        # 轻量模型（示例：ExtraTrees；也可 mutual_info_*）\n",
    "        from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
    "        model = (ExtraTreesClassifier(n_estimators=200, max_features='sqrt', n_jobs=-1, random_state=self.state)\n",
    "                 if is_class else\n",
    "                 ExtraTreesRegressor(n_estimators=200, max_features='sqrt', n_jobs=-1, random_state=self.state))\n",
    "        model.fit(X_train_imp, y_train_imp)\n",
    "    \n",
    "        importances = model.feature_importances_\n",
    "        top_num_feats = pd.Series(importances, index=feat_cols).sort_values(ascending=False).head(top_k).index.tolist()\n",
    "    \n",
    "        print(f\"[select_important_features] 运行耗时: {time.time()-start_time:.2f} 秒（未编码）\")\n",
    "        return top_num_feats\n",
    "\n",
    "\n",
    "            \n",
    "    def new_features(self, data, top_num_feats=None):\n",
    "        # 创建所有数值特征两两组合的乘积特征\n",
    "        feats = top_num_feats if top_num_feats else self.num_features\n",
    "        for c1, c2 in list(combinations(feats, 2)):\n",
    "            data[f\"{c1}_{c2}\"] = data[c1] * data[c2]\n",
    "        # 将类别特征转换为 category 类型\n",
    "        data[self.cat_features] = data[self.cat_features].astype('category')\n",
    "        return data\n",
    "\n",
    "    def log_transformation(self):\n",
    "        # 对目标列做 log1p 变换\n",
    "        self.train[self.target] = np.log1p(self.train[self.target]) \n",
    "        return self\n",
    "        \n",
    "    def remove_outliers(self):\n",
    "        # 基于 IQR 的异常值去除方法\n",
    "        Q1 = self.train[self.target].quantile(0.25)\n",
    "        Q3 = self.train[self.target].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_limit = Q1 - 1.5 * IQR\n",
    "        upper_limit = Q3 + 1.5 * IQR\n",
    "        self.train = self.train[(self.train[self.target] >= lower_limit) & (self.train[self.target] <= upper_limit)]\n",
    "        self.train.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    def missing_values(self):\n",
    "        # 将类别特征中的缺失值替换为字符串 'NaN'\n",
    "        self.train[self.cat_features] = self.train[self.cat_features].fillna('NaN')\n",
    "        self.test[self.cat_features] = self.test[self.cat_features].fillna('NaN')\n",
    "        return self\n",
    "\n",
    "    def reduce_mem(self, df):\n",
    "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64', \"uint16\", \"uint32\", \"uint64\"]\n",
    "        for col in df.columns:\n",
    "        # 修正④：将 dtype 转为字符串再比较\n",
    "            col_type = str(df[col].dtype)\n",
    "            if col_type in numerics:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if \"int\" in col_type:\n",
    "                    if c_min >= np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min >= np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min >= np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.int64)\n",
    "                else:\n",
    "                # 修正⑤：第二个分支用 elif，避免覆盖\n",
    "                    if c_min >= np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min >= np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e039cad6-324d-4b68-910f-5d6b360e55e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[select_important_features] 运行耗时: 46.19 秒（未编码）\n",
      "[特征重要度] 46.37s\n",
      "[交互特征] 47.13s\n",
      "[总耗时] 49.15s\n"
     ]
    }
   ],
   "source": [
    "t = Transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "981d2dd2-7601-4425-8162-28048b85149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_enc, y, test, test_enc, cat_features, num_features = t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bca128a-a271-4657-aa76-7377c7ee4ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(cat_features, num_features):\n",
    "    \n",
    "    x_input_cats = layers.Input(shape=(n_cat,), dtype=\"int32\", name=\"cats\")\n",
    "    embs = []\n",
    "    for j in range(n_cat):\n",
    "        # 取出第 j 列（稳妥写法：Lambda + gather）\n",
    "        col_j = layers.Lambda(lambda x, idx=j: tf.gather(x, indices=idx, axis=1))(x_input_cats)\n",
    "        # 将编码整体 +1，把 0 留作 unknown；input_dim 也 +1 做安全缓冲\n",
    "        col_j = layers.Lambda(lambda z: z + 1)(col_j)\n",
    "\n",
    "        vocab = int(np.ceil(t.cat_features_card[j]))  # 已在 Transform 里统计\n",
    "        emb_dim = int(np.ceil(np.sqrt(max(2, vocab))))  # 简单经验公式，至少 2 维\n",
    "        e = layers.Embedding(input_dim=vocab + 1, output_dim=emb_dim, name=f\"emb_{cat_features[j]}\")\n",
    "        x = e(col_j)                     # (batch, 1, emb_dim)\n",
    "        x = layers.Flatten()(x)          # (batch, emb_dim)\n",
    "        embs.append(x)\n",
    "        \n",
    "    # 拼接：所有 embedding + 数值向量\n",
    "    x = layers.Concatenate(axis=-1)(embs + [x_input_nums])\n",
    "\n",
    "    # MLP\n",
    "    for _ in range(3):\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "    out = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = keras.Model(inputs=[x_input_cats, x_input_nums], outputs=out)\n",
    "\n",
    "    # 指标用显式 AUC\n",
    "    opt = keras.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-4)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy',\n",
    "                  metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a7bd0dc-bc5c-48f0-b96a-a604f714f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import contextlib, io\n",
    "import ydf; ydf.verbose(2)\n",
    "from ydf import RandomForestLearner\n",
    "\n",
    "def YDFClassification(learner_class):\n",
    "\n",
    "    class YDFXClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "        def __init__(self, params=None):\n",
    "            self.params = {} if params is None else params.copy()\n",
    "\n",
    "        def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "            assert isinstance(X, pd.DataFrame)\n",
    "            assert isinstance(y, pd.Series)\n",
    "\n",
    "            self.classes_ = list(y.unique())\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "\n",
    "            target = y.name\n",
    "            params = self.params.copy()\n",
    "            params['label'] = target\n",
    "            params['task'] = ydf.Task.CLASSIFICATION\n",
    "\n",
    "            df = pd.concat([X, y], axis=1)\n",
    "\n",
    "            with contextlib.redirect_stdout(io.StringIO()), \\\n",
    "                 contextlib.redirect_stderr(io.StringIO()):\n",
    "                self.model = learner_class(**params).train(df)\n",
    "\n",
    "            return self\n",
    "\n",
    "        def predict_proba(self, X: pd.DataFrame) -> np.ndarray:\n",
    "            assert isinstance(X, pd.DataFrame)\n",
    "\n",
    "            with contextlib.redirect_stdout(io.StringIO()), \\\n",
    "                 contextlib.redirect_stderr(io.StringIO()):\n",
    "                raw = self.model.predict(X)\n",
    "\n",
    "            proba = np.asarray(raw)\n",
    "            if proba.ndim == 1:\n",
    "                proba = np.vstack([1 - proba, proba]).T\n",
    "\n",
    "            return proba\n",
    "\n",
    "        def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "            proba = self.predict_proba(X)\n",
    "            idx = proba.argmax(axis=1)\n",
    "            return np.array(self.classes_)[idx]\n",
    "\n",
    "    return YDFXClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9041bea-525e-4200-8c68-549aaa702417",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'XGB': XGBClassifier(**{\n",
    "        'tree_method': 'hist',             # 本地CPU\n",
    "        'n_estimators': 10000,\n",
    "        'objective': 'binary:logistic',\n",
    "        'random_state': Config.state,\n",
    "        'eval_metric': 'auc',\n",
    "        'booster': 'gbtree',\n",
    "        'n_jobs': -1,\n",
    "        'reg_lambda': 4.510522889747622,\n",
    "        'reg_alpha': 5.007953193043952,\n",
    "        'colsample_bytree': 0.5831655543160346,\n",
    "        'subsample': 0.9808690492838653,\n",
    "        'learning_rate': 0.008247101477015132,\n",
    "        'max_depth': 11,\n",
    "        'min_child_weight': 1,\n",
    "    }),\n",
    "    'LGBM': LGBMClassifier(**{\n",
    "        'random_state': Config.state,\n",
    "        'verbose': -1,\n",
    "        'n_estimators': 10000,\n",
    "        # 'metric': 'auc',   # 建议在 fit 的 callbacks 里管控早停与评估\n",
    "        'objective': 'binary',\n",
    "        'max_depth': 16,\n",
    "        'learning_rate': 0.007366917567300051,\n",
    "        'min_child_samples': 164,\n",
    "        'subsample': 0.9022880020285295,\n",
    "        'colsample_bytree': 0.4213201532077694,\n",
    "        'num_leaves': 122,\n",
    "        'reg_alpha': 1.083996192298843,\n",
    "        'reg_lambda': 0.0700057221912873,\n",
    "        # 本地CPU：不要 device_type\n",
    "    }),\n",
    "    'LGBM2': LGBMClassifier(**{\n",
    "        'random_state': Config.state,\n",
    "        'verbose': -1,\n",
    "        'n_estimators': 10000,\n",
    "        'objective': 'binary',\n",
    "        'max_depth': 19,\n",
    "        'learning_rate': 0.010196940756517232,\n",
    "        'min_child_samples': 40,\n",
    "        'subsample': 0.5388367974706456,\n",
    "        'colsample_bytree': 0.24506890759293215,\n",
    "        'num_leaves': 360,\n",
    "        'reg_alpha': 0.11493527242956506,\n",
    "        'reg_lambda': 0.8048854866109955,\n",
    "    }),\n",
    "    'CAT': CatBoostClassifier(**{\n",
    "        'random_state': Config.state,\n",
    "        'loss_function': \"Logloss\",\n",
    "        'eval_metric': \"AUC\",\n",
    "        'n_estimators': 5000,\n",
    "        'learning_rate': 0.06524873965257823,\n",
    "        'l2_leaf_reg': 0.8867612905712001,\n",
    "        'bagging_temperature': 0.1317347791955057,\n",
    "        'random_strength': 0.9922857768340815,\n",
    "        'depth': 7,\n",
    "        'min_data_in_leaf': 8,\n",
    "        'task_type': \"CPU\",     # 本地CPU\n",
    "        'verbose': 0,\n",
    "    }),\n",
    "    'CAT2': CatBoostClassifier(**{\n",
    "        'random_state': Config.state,\n",
    "        'loss_function': \"Logloss\",\n",
    "        'eval_metric': \"AUC\",\n",
    "        'n_estimators': 5000,\n",
    "        'learning_rate': 0.034582298874165696,\n",
    "        'l2_leaf_reg': 0.9838795180512044,\n",
    "        'bagging_temperature': 0.22069473702418926,\n",
    "        'random_strength': 1.0557491242401338,\n",
    "        'depth': 9,\n",
    "        'min_data_in_leaf': 166,\n",
    "        'task_type': \"CPU\",     # 本地CPU\n",
    "        'verbose': 0,\n",
    "    }),\n",
    "    'NN': None,  # 占位即可，Trainer 里会 build\n",
    "    'YDF': YDFClassification(RandomForestLearner)({\n",
    "        'num_trees': 1000,\n",
    "        'max_depth': 6,\n",
    "        'random_seed': Config.state,\n",
    "        'growing_strategy': 'BEST_FIRST_GLOBAL'\n",
    "    })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8abfe50-7bcf-4020-a155-f9c185ffda31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(Config):\n",
    "    \n",
    "    def __init__(self, X, X_enc, y, test, test_enc, models, training=True):\n",
    "        self.X = X\n",
    "        self.X_enc = X_enc\n",
    "        self.test = test\n",
    "        self.test_enc = test_enc\n",
    "        self.y = y\n",
    "        self.models = models\n",
    "        self.training = training\n",
    "        self.scores = pd.DataFrame(columns=['Score'])\n",
    "        self.OOF_preds = pd.DataFrame(dtype=float)\n",
    "        self.TEST_preds = pd.DataFrame(dtype=float)\n",
    "        self.folds = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.state)\n",
    "    \n",
    "    def train(self, model, X, y, test, model_name):\n",
    "        oof_pred = np.zeros(X.shape[0], dtype=float)\n",
    "        test_pred = np.zeros(test.shape[0], dtype=float)\n",
    "\n",
    "        print('='*20)\n",
    "        print(model_name)\n",
    "                \n",
    "        for n_fold, (train_id, valid_id) in enumerate(self.folds.split(X, y)):\n",
    "            X_train = X.iloc[train_id].copy()\n",
    "            y_train = y.iloc[train_id]\n",
    "            X_val = X.iloc[valid_id].copy()\n",
    "            y_val = y.iloc[valid_id]\n",
    "            X_test = test.copy()\n",
    "            \n",
    "            \n",
    "            if 'NN' in model_name:\n",
    "                X_train_cats = X_train[cat_features].astype('int32')\n",
    "                X_train_nums = X_train[num_features].astype('float32')\n",
    "                X_val_cats   = X_val[cat_features].astype('int32')\n",
    "                X_val_nums   = X_val[num_features].astype('float32')\n",
    "                X_test_cats  = X_test[cat_features].astype('int32')\n",
    "                X_test_nums  = X_test[num_features].astype('float32')\n",
    "                \n",
    "                model = build_model(cat_features, num_features)                        \n",
    "                keras.utils.set_random_seed(self.state)\n",
    "                optimizer = keras.optimizers.AdamW(learning_rate=1e-2, weight_decay=1e-3)\n",
    "                model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['auc'])\n",
    "                \n",
    "                model.fit([X_train_cats,X_train_nums], y_train, \n",
    "                          validation_data=([X_val_cats, X_val_nums], y_val),\n",
    "                          epochs=20,\n",
    "                          batch_size=1000,\n",
    "                          callbacks=[keras.callbacks.ReduceLROnPlateau(patience=1),\n",
    "                                     keras.callbacks.EarlyStopping(patience=3)\n",
    "                                    ])\n",
    "                y_pred_val = model.predict([X_val_cats, X_val_nums]).squeeze()                      \n",
    "                test_pred += model.predict([X_test_cats, X_test_nums]).squeeze() / self.n_splits             \n",
    "                                      \n",
    "            else:\n",
    "                # XGBoost：用 early_stopping_rounds\n",
    "                if \"XGB\" in model_name:\n",
    "                    model.set_params(early_stopping_rounds=50)  # ← 放到参数里\n",
    "                    model.fit(\n",
    "                        X_train, y_train,\n",
    "                        eval_set=[(X_val, y_val)],\n",
    "                        verbose=False\n",
    "                    )\n",
    "            \n",
    "                # CatBoost：用过拟合检测参数（早停）\n",
    "                elif \"CAT\" in model_name:\n",
    "                    model.set_params(od_type='Iter', od_wait=50)  # ← 50轮不提升就停\n",
    "                    model.fit(\n",
    "                        X_train, y_train,\n",
    "                        eval_set=(X_val, y_val),   # 注意：CatBoost 用 tuple\n",
    "                        verbose=False\n",
    "                    )\n",
    "            \n",
    "                # LightGBM：你原来就有早停（保持不变）\n",
    "                elif \"LGBM\" in model_name:\n",
    "                    model.fit(\n",
    "                        X_train, y_train,\n",
    "                        eval_set=[(X_val, y_val)],\n",
    "                        categorical_feature=cat_features,\n",
    "                        feature_name='auto',\n",
    "                        callbacks=[log_evaluation(0), early_stopping(self.early_stop, verbose=False)]\n",
    "                    )\n",
    "            \n",
    "                # 其他模型：正常拟合\n",
    "                else:\n",
    "                    model.fit(X_train, y_train)\n",
    "                    \n",
    "                y_pred_val = model.predict_proba(X_val)[:, 1]            \n",
    "                test_pred += model.predict_proba(X_test)[:, 1] / self.n_splits\n",
    "                \n",
    "            oof_pred[valid_id] = y_pred_val\n",
    "            score = roc_auc_score(y_val, y_pred_val)\n",
    "            print(score)\n",
    "            self.scores.loc[f'{model_name}', f'Fold {n_fold+1}'] = score                                      \n",
    "\n",
    "        self.scores.loc[f'{model_name}', 'Score'] = self.scores.loc[f'{model_name}'][1:].mean()\n",
    "\n",
    "        return oof_pred, test_pred\n",
    "\n",
    "    def run(self):\n",
    "        for model_name, model in tqdm(self.models.items()):\n",
    "\n",
    "            if self.training:                \n",
    "                use_raw = any(k in model_name for k in ['LGBM', 'CAT', 'HGB', 'YDF'])  # 删掉'XGB'\n",
    "                if use_raw:\n",
    "                    X = self.X.copy()\n",
    "                    test = self.test.copy()\n",
    "                else:\n",
    "                    X = self.X_enc.copy()\n",
    "                    test = self.test_enc.copy()\n",
    "                print(model_name, X.dtypes.value_counts())\n",
    "\n",
    "                oof_pred, test_pred = self.train(model, X, self.y, test, model_name)\n",
    "                pd.DataFrame(oof_pred, columns=[f'{model_name}']).to_csv(f'{model_name}_oof.csv', index=False)\n",
    "                pd.DataFrame(test_pred, columns=[f'{model_name}']).to_csv(f'{model_name}_test.csv', index=False)\n",
    "            \n",
    "            else:\n",
    "                oof_pred = pd.read_csv(f'/kaggle/input/bank-class-models/{model_name}_oof.csv')\n",
    "                test_pred = pd.read_csv(f'/kaggle/input/bank-class-models/{model_name}_test.csv')\n",
    "                for n_fold, (train_id, valid_id) in enumerate(self.folds.split(oof_pred, self.y)):\n",
    "                    y_pred_val, y_val = oof_pred.loc[valid_id], self.y.loc[valid_id]\n",
    "                    self.scores.loc[f'{model_name}', f'Fold {n_fold+1}'] = roc_auc_score(y_val, y_pred_val)\n",
    "                self.scores.loc[f'{model_name}', 'Score'] = self.scores.loc[f'{model_name}'][1:].mean()\n",
    "\n",
    "            self.OOF_preds[f'{model_name}'] = oof_pred\n",
    "            self.TEST_preds[f'{model_name}'] = test_pred\n",
    "            \n",
    "        if len(self.models)>1:\n",
    "            meta_model = LogisticRegression(C = 0.1, random_state = self.state, max_iter = 1000)\n",
    "            self.OOF_preds[\"Ensemble\"], self.TEST_preds[\"Ensemble\"] = self.train(meta_model, self.OOF_preds, y, self.TEST_preds, 'Ensemble')\n",
    "            self.scores = self.scores.sort_values('Score')\n",
    "            self.score_bar()\n",
    "            self.plot_result(self.OOF_preds[\"Ensemble\"])\n",
    "            return self.TEST_preds[\"Ensemble\"]\n",
    "        else:\n",
    "            print(Style.BRIGHT+Fore.GREEN+f'{model_name} score {self.scores.loc[f\"{model_name}\", \"Score\"]:.5f}\\n')\n",
    "            self.plot_result(self.OOF_preds[f'{model_name}'])\n",
    "            return self.TEST_preds[f'{model_name}']\n",
    "\n",
    "    def score_bar(self):\n",
    "        plt.figure(figsize=(18, 6))      \n",
    "        colors = ['#3cb371' if i != 'Ensemble' else 'r' for i in self.scores.Score.index]\n",
    "        hbars = plt.barh(self.scores.index, self.scores.Score, color=colors, height=0.8)\n",
    "        plt.bar_label(hbars, fmt='%.6f')\n",
    "        plt.xlim(0.8, 1)\n",
    "        plt.ylabel('Models')\n",
    "        plt.xlabel('Score')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_result(self, oof):           \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "        for col in self.OOF_preds:\n",
    "            RocCurveDisplay.from_predictions(self.y.sort_index(), self.OOF_preds[col], name=f\"{col}\", ax=axes[0])            \n",
    "        axes[0].plot([0, 1], [0, 1], linestyle='--', lw=2, color='black')\n",
    "        axes[0].set_xlabel('False Positive Rate')\n",
    "        axes[0].set_ylabel('True Positive Rate')\n",
    "        axes[0].set_title('ROC')\n",
    "        axes[0].legend(loc=\"lower right\")\n",
    "        \n",
    "        ConfusionMatrixDisplay.from_predictions(self.y.sort_index(), (oof>=0.5).astype(int), display_labels=self.labels, colorbar=False, ax=axes[1], cmap = 'Greens')\n",
    "        axes[1].set_title('Confusion Matrix')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88901f-89cc-4210-82a2-f1ee35de6909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4a09e23-8fbc-4a3e-9c73-0580275a731f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b2e278ec084a14a3bd1de937e1e77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB float64    36\n",
      "int64       9\n",
      "Name: count, dtype: int64\n",
      "====================\n",
      "XGB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m trainer = Trainer(X, X_enc, y, test, test_enc, models, training = \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m TEST_preds = trainer.run()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36mTrainer.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    109\u001b[39m     test = \u001b[38;5;28mself\u001b[39m.test_enc.copy()\n\u001b[32m    110\u001b[39m \u001b[38;5;28mprint\u001b[39m(model_name, X.dtypes.value_counts())\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m oof_pred, test_pred = \u001b[38;5;28mself\u001b[39m.train(model, X, \u001b[38;5;28mself\u001b[39m.y, test, model_name)\n\u001b[32m    113\u001b[39m pd.DataFrame(oof_pred, columns=[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m]).to_csv(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_oof.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    114\u001b[39m pd.DataFrame(test_pred, columns=[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m]).to_csv(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_test.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, model, X, y, test, model_name)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mXGB\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name:\n\u001b[32m     57\u001b[39m     model.set_params(early_stopping_rounds=\u001b[32m50\u001b[39m)  \u001b[38;5;66;03m# ← 放到参数里\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     model.fit(\n\u001b[32m     59\u001b[39m         X_train, y_train,\n\u001b[32m     60\u001b[39m         eval_set=[(X_val, y_val)],\n\u001b[32m     61\u001b[39m         verbose=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     62\u001b[39m     )\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# CatBoost：用过拟合检测参数（早停）\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCAT\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myproject/lib/python3.11/site-packages/xgboost/core.py:726\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    725\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m726\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myproject/lib/python3.11/site-packages/xgboost/sklearn.py:1531\u001b[39m, in \u001b[36mfit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1524\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m   1525\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mintercept_\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> np.ndarray:\n\u001b[32m   1526\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Intercept (bias) property\u001b[39;00m\n\u001b[32m   1527\u001b[39m \n\u001b[32m   1528\u001b[39m \u001b[33;03m    For tree-based model, the returned value is the `base_score`.\u001b[39;00m\n\u001b[32m   1529\u001b[39m \n\u001b[32m   1530\u001b[39m \u001b[33;03m    Returns\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1531\u001b[39m \u001b[33;03m    -------\u001b[39;00m\n\u001b[32m   1532\u001b[39m \u001b[33;03m    intercept_ : array of shape ``(1,)`` or ``[n_classes]``\u001b[39;00m\n\u001b[32m   1533\u001b[39m \n\u001b[32m   1534\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1535\u001b[39m     booster_config = \u001b[38;5;28mself\u001b[39m.get_xgb_params()[\u001b[33m\"\u001b[39m\u001b[33mbooster\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1536\u001b[39m     b = \u001b[38;5;28mself\u001b[39m.get_booster()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myproject/lib/python3.11/site-packages/xgboost/core.py:726\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    725\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m726\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myproject/lib/python3.11/site-packages/xgboost/training.py:182\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    180\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    181\u001b[39m     bst.update(dtrain, iteration=i, fobj=obj)\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    183\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    185\u001b[39m bst = cb_container.after_training(bst)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myproject/lib/python3.11/site-packages/xgboost/callback.py:258\u001b[39m, in \u001b[36mCallbackContainer.after_iteration\u001b[39m\u001b[34m(self, model, epoch, dtrain, evals)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, name \u001b[38;5;129;01min\u001b[39;00m evals:\n\u001b[32m    257\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m name.find(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m) == -\u001b[32m1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDataset name should not contain `-`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m score: \u001b[38;5;28mstr\u001b[39m = model.eval_set(evals, epoch, \u001b[38;5;28mself\u001b[39m.metric, \u001b[38;5;28mself\u001b[39m._output_margin)\n\u001b[32m    259\u001b[39m metric_score = _parse_eval_str(score)\n\u001b[32m    260\u001b[39m \u001b[38;5;28mself\u001b[39m._update_history(metric_score, epoch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myproject/lib/python3.11/site-packages/xgboost/core.py:2212\u001b[39m, in \u001b[36mBooster.eval_set\u001b[39m\u001b[34m(self, evals, iteration, feval, output_margin)\u001b[39m\n\u001b[32m   2209\u001b[39m evnames = c_array(ctypes.c_char_p, [c_str(d[\u001b[32m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m evals])\n\u001b[32m   2210\u001b[39m msg = ctypes.c_char_p()\n\u001b[32m   2211\u001b[39m _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2212\u001b[39m     _LIB.XGBoosterEvalOneIter(\n\u001b[32m   2213\u001b[39m         \u001b[38;5;28mself\u001b[39m.handle,\n\u001b[32m   2214\u001b[39m         ctypes.c_int(iteration),\n\u001b[32m   2215\u001b[39m         dmats,\n\u001b[32m   2216\u001b[39m         evnames,\n\u001b[32m   2217\u001b[39m         c_bst_ulong(\u001b[38;5;28mlen\u001b[39m(evals)),\n\u001b[32m   2218\u001b[39m         ctypes.byref(msg),\n\u001b[32m   2219\u001b[39m     )\n\u001b[32m   2220\u001b[39m )\n\u001b[32m   2221\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m msg.value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2222\u001b[39m res = msg.value.decode()  \u001b[38;5;66;03m# pylint: disable=no-member\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(X, X_enc, y, test, test_enc, models, training = True)\n",
    "TEST_preds = trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aacb78-a117-4ec3-9565-d6555936c6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
