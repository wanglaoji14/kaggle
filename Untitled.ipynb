{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2eb30fb-a95f-4183-bbdf-72677a8366f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder, label_binarize, OrdinalEncoder, QuantileTransformer, TargetEncoder\n",
    "from category_encoders import CatBoostEncoder, MEstimateEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression, LinearRegression, BayesianRidge, Ridge\n",
    "\n",
    "from sklearn import set_config\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, root_mean_squared_error, mean_squared_error, precision_recall_curve, make_scorer, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, matthews_corrcoef\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "from colorama import Fore, Style, init\n",
    "from copy import deepcopy\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, StratifiedKFold, KFold, RepeatedKFold, cross_val_score, StratifiedGroupKFold\n",
    "import xgboost as xgb\n",
    "from xgboost import DMatrix, XGBClassifier, XGBRegressor\n",
    "\n",
    "from lightgbm import log_evaluation, early_stopping, LGBMClassifier, LGBMRegressor, Dataset\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
    "from tqdm.notebook import tqdm\n",
    "from optuna.samplers import TPESampler, CmaEsSampler\n",
    "from optuna.pruners import HyperbandPruner\n",
    "from functools import partial\n",
    "from IPython.display import display_html, clear_output\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import gc\n",
    "import re\n",
    "from typing import Literal, NamedTuple\n",
    "from itertools import combinations\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a843c72b-cd8b-4ae5-87ff-f26fb593e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#为预处理数据\n",
    "class Config:\n",
    "\n",
    "        state = 42\n",
    "        n_splits = 10\n",
    "        early_stop = 100\n",
    "        \n",
    "        target = 'y'\n",
    "        train = pd.read_csv(\"data/train.csv\")\n",
    "        test = pd.read_csv(\"data/test.csv\")\n",
    "        submission =pd.read_csv(\"data/sample_submission.csv\")\n",
    "    \n",
    "        original_data = False\n",
    "        outliers = False\n",
    "        log_trf = False\n",
    "        feature_eng = True\n",
    "        missing = False\n",
    "        labels = list(train[target].unique())\n",
    "        topk_interactions = 20\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3edfbdf-6da3-4802-bdd7-8dbc872dd831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class Transform(Config):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 调用父类 Config 的 __init__ 方法（如果存在）\n",
    "        super().__init__()\n",
    "        t0 = time.time()\n",
    "        # 如果启用 original_data，则将原始数据合并到训练集\n",
    "        if self.original_data:\n",
    "            start = time.time()\n",
    "            # 将目标列转换为 0/1（假设值为 \"yes\" 和 \"no\"）\n",
    "            self.train_org[self.target] = (self.train_org[self.target] == \"yes\").astype(int)\n",
    "            # 合并并去重\n",
    "            self.train = pd.concat([self.train, self.train_org], ignore_index=True).drop_duplicates()\n",
    "            self.train.reset_index(drop=True, inplace=True)\n",
    "            print(f\"[合并原始数据] {time.time()-start:.2f}s\")\n",
    "\n",
    "        \n",
    "        # 获取数值型特征列名（排除 object/bool/category/string）\n",
    "        self.num_features = self.train.drop(self.target, axis=1)\\\n",
    "            .select_dtypes(exclude=['object', 'bool', 'category', 'string']).columns.tolist()\n",
    "        \n",
    "        # 获取类别特征列名（只保留 object/bool/category/string）\n",
    "        self.cat_features = self.train.drop(self.target, axis=1)\\\n",
    "            .select_dtypes(include=['object', 'bool', 'category', 'string']).columns.tolist()\n",
    "\n",
    "        if self.missing:\n",
    "            self.missing_values()\n",
    "\n",
    "        if self.outliers:\n",
    "            self.remove_outliers()\n",
    "\n",
    "        if self.log_trf:\n",
    "            self.log_transformation()\n",
    "\n",
    "\n",
    "        start = time.time()\n",
    "        self.important_features = self.select_important_features(top_k=20)\n",
    "        print(f\"[特征重要度] {time.time()-start:.2f}s\")\n",
    "\n",
    "        if self.feature_eng and self.important_features:\n",
    "            self.train = self.new_features(self.train, self.important_features)\n",
    "            self.test  = self.new_features(self.test, self.important_features)\n",
    "            self.num_features = self.train.drop(self.target, axis=1)\\\n",
    "                .select_dtypes(exclude=['object', 'bool', 'string', 'category']).columns.tolist()\n",
    "            print(f\"[交互特征] {time.time()-start:.2f}s\")\n",
    "\n",
    "        start = time.time()\n",
    "        self.encode()\n",
    "\n",
    "        print(f\"[总耗时] {time.time()-t0:.2f}s\")\n",
    "        \n",
    "    def __call__(self):\n",
    "        # 保存目标列\n",
    "        self.y = self.train[self.target]\n",
    "        # 保存原始特征\n",
    "        self.X = self.train.drop(self.target, axis=1)\n",
    "        # 保存编码后的特征\n",
    "        self.X_enc = self.train_enc.drop(self.target, axis=1)\n",
    "        return self.X, self.X_enc, self.y, self.test, self.test_enc, self.cat_features, self.num_features\n",
    "    \n",
    "    def encode(self):\n",
    "        self.train_enc = self.train.copy()\n",
    "        self.test_enc = self.test.copy()\n",
    "        \n",
    "        self.cat_features_card = []\n",
    "        for f in self.cat_features:\n",
    "            self.cat_features_card.append(self.train[f].nunique())\n",
    "        \n",
    "        # 创建编码器并仅用训练集类别特征进行 fit\n",
    "        oe = OrdinalEncoder()\n",
    "        oe.fit(self.train_enc[self.cat_features])\n",
    "        \n",
    "        # 分别对训练集和测试集进行 transform\n",
    "        self.train_enc[self.cat_features] = oe.transform(self.train_enc[self.cat_features]).astype(int)\n",
    "        self.test_enc[self.cat_features] = oe.transform(self.test_enc[self.cat_features]).astype(int)\n",
    "        \n",
    "        # 创建标准化器并仅用训练集数值特征进行 fit\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(self.train_enc[self.num_features])\n",
    "\n",
    "        \n",
    "        # 分别对训练集和测试集进行 transform\n",
    "        self.train_enc[self.num_features] = scaler.transform(self.train_enc[self.num_features])\n",
    "        self.test_enc[self.num_features] = scaler.transform(self.test_enc[self.num_features])\n",
    "\n",
    "    def select_important_features(self, top_k=20, task='auto'):\n",
    "        \"\"\"\n",
    "        基于树模型的特征重要度筛选，并记录耗时。\n",
    "        功能：返回用于生成交互项的前 top_k 个重要数值特征。\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Step 1: 构造候选列\n",
    "        feat_cols = [c for c in self.num_features if c in self.train.columns and c != self.target]\n",
    "        if not feat_cols:\n",
    "            print(\"[select_important_features] 无可用数值特征，返回空。\")\n",
    "            return []\n",
    "\n",
    "\n",
    "        # Step 2: 准备训练数据\n",
    "        X_train_imp = self.train[feat_cols]\n",
    "        y_train_imp = self.train[self.target]\n",
    "        is_class = (y_train_imp.nunique() <= 10)\n",
    "\n",
    "        # 轻量模型（示例：ExtraTrees；也可 mutual_info_*）\n",
    "        from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
    "        model = (ExtraTreesClassifier(n_estimators=200, max_features='sqrt', n_jobs=-1, random_state=self.state)\n",
    "                 if is_class else\n",
    "                 ExtraTreesRegressor(n_estimators=200, max_features='sqrt', n_jobs=-1, random_state=self.state))\n",
    "        model.fit(X_train_imp, y_train_imp)\n",
    "    \n",
    "        importances = model.feature_importances_\n",
    "        top_num_feats = pd.Series(importances, index=feat_cols).sort_values(ascending=False).head(top_k).index.tolist()\n",
    "    \n",
    "        print(f\"[select_important_features] 运行耗时: {time.time()-start_time:.2f} 秒（未编码）\")\n",
    "        return top_num_feats\n",
    "\n",
    "\n",
    "            \n",
    "    def new_features(self, data, top_num_feats=None):\n",
    "        # 创建所有数值特征两两组合的乘积特征\n",
    "        feats = top_num_feats if top_num_feats else self.num_features\n",
    "        for c1, c2 in list(combinations(feats, 2)):\n",
    "            data[f\"{c1}_{c2}\"] = data[c1] * data[c2]\n",
    "        # 将类别特征转换为 category 类型\n",
    "        data[self.cat_features] = data[self.cat_features].astype('category')\n",
    "        return data\n",
    "\n",
    "    def log_transformation(self):\n",
    "        # 对目标列做 log1p 变换\n",
    "        self.train[self.target] = np.log1p(self.train[self.target]) \n",
    "        return self\n",
    "        \n",
    "    def remove_outliers(self):\n",
    "        # 基于 IQR 的异常值去除方法\n",
    "        Q1 = self.train[self.target].quantile(0.25)\n",
    "        Q3 = self.train[self.target].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_limit = Q1 - 1.5 * IQR\n",
    "        upper_limit = Q3 + 1.5 * IQR\n",
    "        self.train = self.train[(self.train[self.target] >= lower_limit) & (self.train[self.target] <= upper_limit)]\n",
    "        self.train.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    def missing_values(self):\n",
    "        # 将类别特征中的缺失值替换为字符串 'NaN'\n",
    "        self.train[self.cat_features] = self.train[self.cat_features].fillna('NaN')\n",
    "        self.test[self.cat_features] = self.test[self.cat_features].fillna('NaN')\n",
    "        return self\n",
    "\n",
    "    def reduce_mem(self, df):\n",
    "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64', \"uint16\", \"uint32\", \"uint64\"]\n",
    "        for col in df.columns:\n",
    "        # 修正④：将 dtype 转为字符串再比较\n",
    "            col_type = str(df[col].dtype)\n",
    "            if col_type in numerics:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if \"int\" in col_type:\n",
    "                    if c_min >= np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min >= np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min >= np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.int64)\n",
    "                else:\n",
    "                # 修正⑤：第二个分支用 elif，避免覆盖\n",
    "                    if c_min >= np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min >= np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e039cad6-324d-4b68-910f-5d6b360e55e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[select_important_features] 运行耗时: 46.19 秒（未编码）\n",
      "[特征重要度] 46.37s\n",
      "[交互特征] 47.13s\n",
      "[总耗时] 49.15s\n"
     ]
    }
   ],
   "source": [
    "t = Transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "981d2dd2-7601-4425-8162-28048b85149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_enc, y, test, test_enc, cat_features, num_features = t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bca128a-a271-4657-aa76-7377c7ee4ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(cat_features, num_features):\n",
    "    \n",
    "    x_input_cats = layers.Input(shape=(len(cat_features),))\n",
    "    embs = []\n",
    "    for j in range(len(cat_features)):\n",
    "        e = layers.Embedding(t.cat_features_card[j], int(np.ceil(np.sqrt(t.cat_features_card[j]))))\n",
    "        x = e(x_input_cats[:,j])\n",
    "        x = layers.Flatten()(x)\n",
    "        embs.append(x)\n",
    "        \n",
    "    x_input_nums = layers.Input(shape=(len(num_features),))\n",
    "    \n",
    "    x = layers.Concatenate(axis=-1)(embs+[x_input_nums]) \n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(.3)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(.3)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(.3)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a7bd0dc-bc5c-48f0-b96a-a604f714f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import contextlib, io\n",
    "import ydf; ydf.verbose(2)\n",
    "from ydf import RandomForestLearner\n",
    "\n",
    "def YDFClassification(learner_class):\n",
    "\n",
    "    class YDFXClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "        def __init__(self, params=None):\n",
    "            self.params = {} if params is None else params.copy()\n",
    "\n",
    "        def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "            assert isinstance(X, pd.DataFrame)\n",
    "            assert isinstance(y, pd.Series)\n",
    "\n",
    "            self.classes_ = list(y.unique())\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "\n",
    "            target = y.name\n",
    "            params = self.params.copy()\n",
    "            params['label'] = target\n",
    "            params['task'] = ydf.Task.CLASSIFICATION\n",
    "\n",
    "            df = pd.concat([X, y], axis=1)\n",
    "\n",
    "            with contextlib.redirect_stdout(io.StringIO()), \\\n",
    "                 contextlib.redirect_stderr(io.StringIO()):\n",
    "                self.model = learner_class(**params).train(df)\n",
    "\n",
    "            return self\n",
    "\n",
    "        def predict_proba(self, X: pd.DataFrame) -> np.ndarray:\n",
    "            assert isinstance(X, pd.DataFrame)\n",
    "\n",
    "            with contextlib.redirect_stdout(io.StringIO()), \\\n",
    "                 contextlib.redirect_stderr(io.StringIO()):\n",
    "                raw = self.model.predict(X)\n",
    "\n",
    "            proba = np.asarray(raw)\n",
    "            if proba.ndim == 1:\n",
    "                proba = np.vstack([1 - proba, proba]).T\n",
    "\n",
    "            return proba\n",
    "\n",
    "        def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "            proba = self.predict_proba(X)\n",
    "            idx = proba.argmax(axis=1)\n",
    "            return np.array(self.classes_)[idx]\n",
    "\n",
    "    return YDFXClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9041bea-525e-4200-8c68-549aaa702417",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'XGB': XGBClassifier(**{'tree_method': 'hist',\n",
    "                            'n_estimators': 10000,\n",
    "                            'objective': 'binary:logistic',\n",
    "                            'random_state': Config.state,\n",
    "                            'eval_metric': 'auc',\n",
    "                            'booster': 'gbtree',\n",
    "                            'n_jobs': -1,\n",
    "                            'reg_lambda': 4.510522889747622,\n",
    "                            'reg_alpha': 5.007953193043952, \n",
    "                            'colsample_bytree': 0.5831655543160346,\n",
    "                            'subsample': 0.9808690492838653,\n",
    "                            'learning_rate': 0.008247101477015132,\n",
    "                            'max_depth': 11,\n",
    "                            'min_child_weight': 1,\n",
    "                            'device': 'cuda',\n",
    "                            }),\n",
    "    'LGBM': LGBMClassifier(**{'random_state': Config.state,\n",
    "                              'verbose': -1,\n",
    "                              'n_estimators': 10000,\n",
    "                              'metric': 'AUC',\n",
    "                              'objective': 'binary',\n",
    "                              'max_depth': 16,\n",
    "                              'learning_rate': 0.007366917567300051,\n",
    "                              'min_child_samples': 164,\n",
    "                              'subsample': 0.9022880020285295,\n",
    "                              'colsample_bytree': 0.4213201532077694,\n",
    "                              'num_leaves': 122, \n",
    "                              'reg_alpha': 1.083996192298843,\n",
    "                              'reg_lambda': 0.0700057221912873,\n",
    "                              'device_type': 'gpu', \n",
    "                              }),\n",
    "    'LGBM2': LGBMClassifier(**{'random_state': Config.state,\n",
    "                               'verbose': -1,\n",
    "                               'n_estimators': 10000,\n",
    "                               'metric': 'AUC',\n",
    "                               'objective': 'binary',\n",
    "                               'max_depth': 19,\n",
    "                               'learning_rate': 0.010196940756517232,\n",
    "                               'min_child_samples': 40,\n",
    "                               'subsample': 0.5388367974706456,\n",
    "                               'colsample_bytree': 0.24506890759293215,\n",
    "                               'num_leaves': 360, \n",
    "                               'reg_alpha': 0.11493527242956506,\n",
    "                               'reg_lambda': 0.8048854866109955,\n",
    "                               'device_type': 'gpu', \n",
    "                              }),\n",
    "    'CAT': CatBoostClassifier(**{'random_state': Config.state,\n",
    "                                 'eval_metric': \"Logloss\",\n",
    "                                 'n_estimators' : 5000,\n",
    "                                 'learning_rate': 0.06524873965257823,\n",
    "                                 'l2_leaf_reg': 0.8867612905712001,\n",
    "                                 'bagging_temperature': 0.1317347791955057,\n",
    "                                 'random_strength': 0.9922857768340815,\n",
    "                                 'depth': 7,\n",
    "                                 'min_data_in_leaf': 8,\n",
    "                                 'task_type': \"GPU\",\n",
    "                                 }),\n",
    "    'CAT2': CatBoostClassifier(**{'random_state': Config.state,\n",
    "                                  'eval_metric': \"Logloss\",\n",
    "                                  'n_estimators' : 5000,\n",
    "                                  'learning_rate': 0.034582298874165696,\n",
    "                                  'l2_leaf_reg': 0.9838795180512044,\n",
    "                                  'bagging_temperature': 0.22069473702418926,\n",
    "                                  'random_strength': 1.0557491242401338,\n",
    "                                  'depth': 9,\n",
    "                                  'min_data_in_leaf': 166,\n",
    "                                  'task_type': \"GPU\"\n",
    "                                 }),\n",
    "    'NN': _,\n",
    "    'YDF': YDFClassification(RandomForestLearner)({'num_trees': 1000,\n",
    "                                                   'max_depth': 6,\n",
    "                                                   'random_seed': Config.state,\n",
    "                                                   'growing_strategy': 'BEST_FIRST_GLOBAL'\n",
    "                                               })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8abfe50-7bcf-4020-a155-f9c185ffda31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(Config):\n",
    "    def __init__(self, X, X_enc, y, test, test_enc, models, training=True):  \n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.X_enc = X_enc\n",
    "        self.test = test\n",
    "        self.test_enc = test_enc\n",
    "        self.y = y\n",
    "        self.models = models  # 传入的模型字典\n",
    "        self.training = training  # 是否进行训练，False 时读取已保存的预测结果\n",
    "        self.scores = pd.DataFrame(columns=['Score'])  # 保存各模型的得分\n",
    "        self.OOF_preds = pd.DataFrame(dtype=float)  # 保存 Out-Of-Fold 预测结果\n",
    "        self.TEST_preds = pd.DataFrame(dtype=float)  # 保存测试集预测结果\n",
    "        # 定义交叉验证方式：分层 K 折，保证类别分布一致\n",
    "        self.folds = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.state)\n",
    "\n",
    "    def train(self, model, X, y, test, model_name):\n",
    "        # 初始化 OOF 预测和测试集预测\n",
    "        oof_pred = np.zeros(X.shape[0], dtype=float)\n",
    "        test_pred = np.zeros(test.shape[0], dtype=float)\n",
    "\n",
    "        print('='*20)\n",
    "        print(model_name)\n",
    "\n",
    "        # 进行 K 折交叉验证\n",
    "        for n_fold, (train_id, valid_id) in enumerate(self.folds.split(X, y)):\n",
    "            # 按照索引切分训练集和验证集\n",
    "            X_train = X.iloc[train_id].copy() if isinstance(X, pd.DataFrame) else X[train_id]\n",
    "            y_train = y.iloc[train_id]\n",
    "            X_val   = X.iloc[valid_id].copy() if isinstance(X, pd.DataFrame) else X[valid_id]\n",
    "            y_val   = y.iloc[valid_id]\n",
    "            X_test  = test.copy()\n",
    "\n",
    "\n",
    "\n",
    "            # 如果是神经网络（NN），需要单独处理类别特征和数值特征\n",
    "            if 'NN' in model_name:\n",
    "                X_train_cats = X_train[cat_features]\n",
    "                X_train_nums = X_train[num_features]\n",
    "\n",
    "                X_val_cats = X_val[cat_features]\n",
    "                X_val_nums = X_val[num_features]\n",
    "\n",
    "                X_test_cats = X_test[cat_features]\n",
    "                X_test_nums = X_test[num_features]\n",
    "\n",
    "                # 构建神经网络模型\n",
    "                model = build_model(cat_features, num_features)\n",
    "                keras.utils.set_random_seed(self.state)\n",
    "                optimizer = keras.optimizers.AdamW(learning_rate=1e-2, weight_decay=1e-3)\n",
    "                model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['auc'])\n",
    "\n",
    "                # 训练神经网络\n",
    "                model.fit([X_train_cats, X_train_nums], y_train,\n",
    "                          validation_data=([X_val_cats, X_val_nums], y_val),\n",
    "                          epochs=20,\n",
    "                          batch_size=1000,\n",
    "                          callbacks=[keras.callbacks.ReduceLROnPlateau(patience=1),\n",
    "                                     keras.callbacks.EarlyStopping(patience=3)])\n",
    "\n",
    "                # 验证集预测\n",
    "                y_pred_val = model.predict([X_val_cats, X_val_nums]).squeeze()\n",
    "                # 测试集预测（取 K 折平均）\n",
    "                test_pred += model.predict([X_test_cats, X_test_nums]).squeeze() / self.n_splits\n",
    "\n",
    "            else:\n",
    "                # XGBoost 模型\n",
    "                if \"XGB\" in model_name:\n",
    "                    # 1) 保证 X_train/X_val/X_test 是数值型（run() 已经给 XGB 走 self.X_enc/self.test_enc）\n",
    "                    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "                    dvalid = xgb.DMatrix(X_val,   label=y_val)\n",
    "                    dtest  = xgb.DMatrix(X_test)\n",
    "                \n",
    "                    # 2) 从 sklearn 的 XGBClassifier 取出超参，映射到原生 params\n",
    "                    p = model.get_params()\n",
    "                    params = {\n",
    "                        \"objective\":          \"binary:logistic\",\n",
    "                        \"eval_metric\":        \"auc\",\n",
    "                        \"tree_method\":        p.get(\"tree_method\", \"hist\"),\n",
    "                        \"device\":             p.get(\"device\", \"cpu\"),       # Kaggle GPU 则 \"cuda\"\n",
    "                        \"max_depth\":          p.get(\"max_depth\", 6),\n",
    "                        \"learning_rate\":      p.get(\"learning_rate\", 0.1),\n",
    "                        \"subsample\":          p.get(\"subsample\", 1.0),\n",
    "                        \"colsample_bytree\":   p.get(\"colsample_bytree\", 1.0),\n",
    "                        \"reg_lambda\":         p.get(\"reg_lambda\", 1.0),\n",
    "                        \"reg_alpha\":          p.get(\"reg_alpha\", 0.0),\n",
    "                        \"min_child_weight\":   p.get(\"min_child_weight\", 1),\n",
    "                        # 如需其它超参，同理从 p 中取并加入\n",
    "                    }\n",
    "                    num_boost_round = p.get(\"n_estimators\", 1000)\n",
    "                \n",
    "                    # 3) 训练 + 早停（以验证集为监控，AUC 越大越好）\n",
    "                    booster = xgb.train(\n",
    "                        params=params,\n",
    "                        dtrain=dtrain,\n",
    "                        num_boost_round=num_boost_round,\n",
    "                        evals=[(dvalid, \"valid\")],\n",
    "                        early_stopping_rounds=self.early_stop,\n",
    "                        verbose_eval=False\n",
    "                    )\n",
    "                \n",
    "                    # 4) 用 best_iteration 做预测（与你原先 y_pred_val / test_pred 的接口保持一致）\n",
    "                    y_pred_val = booster.predict(dvalid, iteration_range=(0, booster.best_iteration + 1))\n",
    "                    test_pred += booster.predict(dtest,  iteration_range=(0, booster.best_iteration + 1)) / self.n_splits\n",
    "\n",
    "                # CatBoost 模型\n",
    "                elif \"CAT\" in model_name:\n",
    "                    try:\n",
    "                        from catboost import Pool\n",
    "                        train_pool = Pool(X_train, y_train, cat_features=cat_features)\n",
    "                        valid_pool = Pool(X_val,   y_val,   cat_features=cat_features)\n",
    "                        model.fit(train_pool,\n",
    "                                  eval_set=valid_pool,\n",
    "                                  early_stopping_rounds=self.early_stop,\n",
    "                                  verbose=False)\n",
    "                    except Exception:\n",
    "                        model.fit(X_train, y_train,\n",
    "                                  eval_set=[(X_val, y_val)],\n",
    "                                  early_stopping_rounds=self.early_stop,\n",
    "                                  verbose=False)\n",
    "\n",
    "                # LightGBM 模型\n",
    "                elif \"LGBM\" in model_name:\n",
    "                    model.fit(X_train, y_train,\n",
    "                              eval_set=[(X_val, y_val)],\n",
    "                              eval_metric='auc',\n",
    "                              early_stopping_rounds=self.early_stop,\n",
    "                              verbose=False)\n",
    "                # 其他模型（如逻辑回归）\n",
    "                else:\n",
    "                    model.fit(X_train, y_train)\n",
    "\n",
    "                # 验证集预测（取预测概率的正类部分）\n",
    "                y_pred_val = model.predict_proba(X_val)[:, 1]\n",
    "                # 测试集预测（取 K 折平均）\n",
    "                test_pred += model.predict_proba(X_test)[:, 1] / self.n_splits\n",
    "\n",
    "            # 保存 OOF 预测结果\n",
    "            oof_pred[valid_id] = y_pred_val\n",
    "            # 计算本折 ROC AUC 分数\n",
    "            score = roc_auc_score(y_val, y_pred_val)\n",
    "            print(score)\n",
    "            self.scores.loc[f'{model_name}', f'Fold {n_fold+1}'] = score\n",
    "\n",
    "        # 计算该模型的平均分\n",
    "        fold_cols = [c for c in self.scores.columns if c.startswith('Fold ')]\n",
    "        self.scores.loc[f'{model_name}', 'Score'] = self.scores.loc[f'{model_name}', fold_cols].astype(float).mean()\n",
    "\n",
    "        return oof_pred, test_pred\n",
    "\n",
    "    def run(self):\n",
    "        # 遍历每个模型，进行训练或读取结果\n",
    "        for model_name, model in tqdm(self.models.items()):\n",
    "\n",
    "            if self.training:                \n",
    "                if 'CAT' in model_name:\n",
    "                  X = self.X.copy()\n",
    "                  test = self.test.copy()\n",
    "                else:\n",
    "                  X = self.X_enc.copy()\n",
    "                  test = self.test_enc.copy() \n",
    "\n",
    "                # 训练模型并保存预测结果\n",
    "                oof_pred, test_pred = self.train(model, X, self.y, test, model_name)\n",
    "                pd.DataFrame(oof_pred, columns=[f'{model_name}']).to_csv(f'{model_name}_oof.csv', index=False)\n",
    "                pd.DataFrame(test_pred, columns=[f'{model_name}']).to_csv(f'{model_name}_test.csv', index=False)\n",
    "\n",
    "            else:\n",
    "                # 如果不训练，则从文件读取 OOF 和测试预测结果\n",
    "                oof_pred  = pd.read_csv(f'/kaggle/input/bank-class-models/{model_name}_oof.csv')[f'{model_name}'].values\n",
    "                test_pred = pd.read_csv(f'/kaggle/input/bank-class-models/{model_name}_test.csv')[f'{model_name}'].values\n",
    "                for n_fold, (train_id, valid_id) in enumerate(self.folds.split(oof_pred, self.y)):\n",
    "                    y_pred_val = oof_pred[valid_id]\n",
    "                    y_val      = self.y.iloc[valid_id]\n",
    "                    self.scores.loc[f'{model_name}', f'Fold {n_fold+1}'] = roc_auc_score(y_val, y_pred_val)\n",
    "                fold_cols = [c for c in self.scores.columns if c.startswith('Fold ')]\n",
    "                self.scores.loc[f'{model_name}', 'Score'] = self.scores.loc[f'{model_name}', fold_cols].astype(float).mean()\n",
    "\n",
    "            # 将 OOF 和测试预测结果保存到类属性中\n",
    "            self.OOF_preds[f'{model_name}']  = oof_pred\n",
    "            self.TEST_preds[f'{model_name}'] = test_pred\n",
    "\n",
    "        # 如果有多个模型，则进行二层 stacking（元模型逻辑回归）\n",
    "        if len(self.models) > 1:\n",
    "            meta_model = LogisticRegression(C=0.1, random_state=self.state, max_iter=1000)\n",
    "            self.OOF_preds[\"Ensemble\"], self.TEST_preds[\"Ensemble\"] = self.train(\n",
    "                meta_model, self.OOF_preds, self.y, self.TEST_preds, 'Ensemble'\n",
    "            )\n",
    "            # 排序模型分数并绘图\n",
    "            self.scores = self.scores.sort_values('Score')\n",
    "            self.score_bar()\n",
    "            self.plot_result(self.OOF_preds[\"Ensemble\"])\n",
    "            return self.TEST_preds[\"Ensemble\"]\n",
    "        else:\n",
    "            # 只有一个模型时，直接输出分数和绘图\n",
    "            only = list(self.models.keys())[0]\n",
    "            print(f'{only} score {self.scores.loc[f\"{only}\", \"Score\"]:.5f}\\n')\n",
    "            self.plot_result(self.OOF_preds[f'{only}'])\n",
    "            return self.TEST_preds[f'{only}']\n",
    "\n",
    "    def score_bar(self):\n",
    "        # 绘制各模型分数的柱状图\n",
    "        plt.figure(figsize=(18, 6))\n",
    "        colors = ['#3cb371' if i != 'Ensemble' else 'r' for i in self.scores.Score.index]\n",
    "        hbars = plt.barh(self.scores.index, self.scores.Score.astype(float), color=colors, height=0.8)\n",
    "        plt.bar_label(hbars, fmt='%.6f')\n",
    "        plt.xlim(0.8, 1)\n",
    "        plt.ylabel('Models')\n",
    "        plt.xlabel('Score')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_result(self, oof):\n",
    "        # 绘制 ROC 曲线和混淆矩阵\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "        for col in self.OOF_preds:\n",
    "            RocCurveDisplay.from_predictions(self.y.sort_index(), self.OOF_preds[col], name=f\"{col}\", ax=axes[0])\n",
    "        axes[0].plot([0, 1], [0, 1], linestyle='--', lw=2, color='black')\n",
    "        axes[0].set_xlabel('False Positive Rate')\n",
    "        axes[0].set_ylabel('True Positive Rate')\n",
    "        axes[0].set_title('ROC')\n",
    "        axes[0].legend(loc=\"lower right\")\n",
    "\n",
    "        ConfusionMatrixDisplay.from_predictions(self.y.sort_index(), (oof>=0.5).astype(int),\n",
    "                                                display_labels=self.labels, colorbar=False, ax=axes[1], cmap='Greens')\n",
    "        axes[1].set_title('Confusion Matrix')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a09e23-8fbc-4a3e-9c73-0580275a731f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10e46e7899a4fcd8cb9ff5609da2b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "XGB\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(X, X_enc, y, test, test_enc, models, training = True)\n",
    "TEST_preds = trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aacb78-a117-4ec3-9565-d6555936c6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
